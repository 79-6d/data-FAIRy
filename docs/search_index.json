[["index.html", "Data FAIRy About How to build this book locally", " Data FAIRy Anton Van de Putte, Yi-Ming Gan, Maxime Sweetlove 2021-11-25 About This document serves to guide researchers who wish to manage the biological data from their scientific expedition Issues or suggestions about the content can be raised through the issue tracker (GitHub account required). How to build this book locally This material has been written using bookdown and R. The dependencies of this project is managed using renv. To build the book locally, clone the repo. If dependencies are not automatically installed by renv when you open data-fairy.Rproj, try the following command. renv::restore() then run the following lines to build the book: library(here) bookdown::render_book(here(&quot;index.Rmd&quot;)) And view it with: browseURL(&quot;docs/index.html&quot;) "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction The biodiversity.aq team has been helping researchers to clean and publish biodiversity data for years. We noticed that data management has been a challenge to everyone and we believe a little improvement in data management could save a lot of time. Hence, we tried to compile a series of concepts that we think are helpful for scientists to manage their data. "],["organization.html", "Chapter 2 Organization 2.1 Project structure 2.2 Naming files 2.3 Organizational tips", " Chapter 2 Organization This section is all about organizing files and folders, which include key points from the following resources: Cookiecutter Data Science Jenny Bryan’s slides about organization Jenny Bryan’s slides about naming things 2.1 Project structure Before we begin data entry, it is important to have an easy-to-manage directory structure to store files at appropriate location. 2.1.1 Why does it matter to have a good project structure? A clear self-documenting project structure helps newcomer to understand an analyses without having to read extensive documentation or all of the code to look for specific things. README’s are great, but if it can be made self-documenting, it does not need to be documented. An example of good project structure adapted from Python’s version from Cookiecutter Data Science is shown below. ├── README.md &lt;- The top-level README. ├── data │ ├── external &lt;- Data from third party sources. │ ├── interim &lt;- Intermediate data that has been transformed. │ ├── processed &lt;- The final, canonical data sets for modeling. │ └── raw &lt;- The original, immutable data dump. │ ├── references &lt;- Data dictionaries, manuals, and all other explanatory materials. │ ├── src &lt;- Source code for use in this project. │ ├── data &lt;- Scripts to download or generate data. │ │ └── make_dataset.R │ │ │ ├── clean &lt;- Scripts to clean data. │ │ └── clean_dataset.R │ │ │ └── visualization &lt;- Scripts to create exploratory and results oriented visualizations │ └── visualize.R └── reports &lt;- Generated QC reports. └── figures &lt;- Generated graphics and figures to be used in reporting. 2.1.2 Why is this a good project structure? This is a good project structure because: Self explanatory: The file name and location are very informative about what it is, why it exists, how it relates to other files/directories. It reflects the inputs, outputs and the flow of information. 2.2 Naming files Having good file names would give you an idea about the information you need about the file. Jenny Bryan has good tips in naming files summarized below. Good file names are: machine readable human readable orderable Good names 2021-05-20_antarctic-penguins.txt 001-248231_myctophidae-gill.jpg southern-ocean-jellyfish.docx File names to avoid 1.txt thesis final_Final_FINAL.docx nJ7UyiE*.txt ça va.txt 2.2.1 Machine readable Machine readable file names will enable you to easily search for a file or a group of files (globbing) easily using regular expression. A regular expression is a sequence of characters that specifies a search pattern. (Wikipedia) To be regular expression and globbing friendly, file names should: Avoid: special characters spaces punctuation accented characters case sensitivity Use: delimiters Examples: library(here) list.files(path = here(&quot;examples/organization_example-file-names/&quot;)) ## [1] &quot;2020-12-11_admiralty-bay_amphipod-trap.txt&quot; &quot;2020-12-11_admiralty-bay_van-veen.txt&quot; &quot;2020-12-14_maxwell-bay_amphipod-trap.txt&quot; ## [4] &quot;2020-12-14_maxwell-bay_ikmt.txt&quot; &quot;2020-12-16_bransfield-strait_van-veen.txt&quot; ls is a command to list all files within the current (~/Desktop/projects/01_cruise-reports/) directory Using globbing/regular expression to narrow file listing that which contains the word “van-veen”: list.files(path = here(&quot;examples/organization_example-file-names/&quot;), pattern = &quot;van-veen&quot;) ## [1] &quot;2020-12-11_admiralty-bay_van-veen.txt&quot; &quot;2020-12-16_bransfield-strait_van-veen.txt&quot; Delimiting the file names also helps to delimit the units of metadata in the file names. For example, the file names above follow the pattern: &lt;date&gt;_&lt;sampling-station&gt;_&lt;sampling-protocol&gt;.&lt;file-extension&gt; _ underscore delimits units of metadata - hyphen delimits words for readability file_list &lt;- list.files(path = here(&quot;examples/organization_example-file-names/&quot;)) tbl &lt;- stringr::str_split_fixed(file_list, &quot;[_\\\\.]&quot;, 4) colnames(tbl) &lt;- c(&quot;date&quot;, &quot;sampling-station&quot;, &quot;sampling-protocol&quot;, &quot;file-type&quot;) tbl ## date sampling-station sampling-protocol file-type ## [1,] &quot;2020-12-11&quot; &quot;admiralty-bay&quot; &quot;amphipod-trap&quot; &quot;txt&quot; ## [2,] &quot;2020-12-11&quot; &quot;admiralty-bay&quot; &quot;van-veen&quot; &quot;txt&quot; ## [3,] &quot;2020-12-14&quot; &quot;maxwell-bay&quot; &quot;amphipod-trap&quot; &quot;txt&quot; ## [4,] &quot;2020-12-14&quot; &quot;maxwell-bay&quot; &quot;ikmt&quot; &quot;txt&quot; ## [5,] &quot;2020-12-16&quot; &quot;bransfield-strait&quot; &quot;van-veen&quot; &quot;txt&quot; 2.2.2 Human readable File name that tells you about the file content saves you time. Similarly, using delimiters as mentioned above helps to make the file names more readable. These file names contain the same information but delimited differently: Without delimiters, the name is hard to read. 20201211admiraltyBayVanVeen.txt Underscores _ to delimit units of metadata. That’s better! 20201211_admiraltybay_vanveen.txt Underscores _ to delimit units of metadata and hyphen - to separate words for readability. Even better! 2020-12-11_admiralty-bay_van-veen.txt 2.2.3 Orderable File names that start with numbers. ISO 8601 standard for dates. left pad other numbers with zero(s). Meaningful names start with numbers allow files to be sorted chronologically. Note that date is in ISO 8601 standard format (YYYY-MM-DD). 2020-01-14_notes.txt 2020-02-21_notes.txt 2020-02-22_notes.txt 2020-03-16_notes.txt If date is in format such as (DD-MM-YYYY), sorting the files does not provide chronological order of events. 14-01-2020_notes.txt 16-03-2020_notes.txt # notes from March comes before February&#39;s 21-02-2020_notes.txt 22-02-2020_notes.txt If files are not meaningful when ordered with date, they can be named with numeric characters first to be able to order them sequentially. For instance, a folder of images to be added into another document following a certain sequence. 001_myctophidae_diaphus-adenomus.jpg 002_myctophidae_diaphus-agassizii.jpg ... 010_myctophidae_diaphus-danae.jpg 011_myctophidae_diaphus-fragilis.jpg If the file names are not left pad with zeros, the order will not be chronological as depicted in the example below. 10_myctophidae_diaphus-danae.jpg 1_myctophidae_diaphus-adenomus.jpg 11_myctophidae_diaphus-fragilis.jpg 2_myctophidae_diaphus-agassizii.jpg 2.3 Organizational tips Here are a couple of quick tips from Jenny Bryan’s organization slides that help to keep your files and folders organised aside from the tips mentioned above: A quarantine directory Revoke write permission to raw data files A prose directory 2.3.1 A quarantine directory If your collaborator send you data with space-containing file names, data in spreadsheet etc that do not fits your standard naming system and practice, you can place those files in a quarantine directory. The renamed or exported plain text files can be move to your data directory. Record what you did in a README or comments in your code to remind yourself about the file’s source, if it is from the outside world in a state that is not ready for your programmatic analysis. 2.3.2 Revoke write permission to raw data files Revoking write permission to raw data files avoid the files to be be accidentally edited by you or someone else. 2.3.3 A prose directory Sometimes you need a folder to keep key emails, internal documentation, explanations or random documents received. Similar to the quarantine directory, the prose directory can be used to park these things without having to keep the same standard for file names and open formats. "],["tidy-data.html", "Chapter 3 Tidy data 3.1 Why tidy data? 3.2 Common spreadsheets mistakes", " Chapter 3 Tidy data This section of the document compiled information from several references: R for Data Science Tidy Data by Hadley Wickham Data Organization in Spreadsheets for Ecologists Data Organization in Spreadsheets Nine simple ways to make it easier to (re)use your data. Tidy data is a standard way of mapping the meaning of a dataset to its structure. In tidy data: Columns = variables Rows = observations Cells = data (values) Figure: Rules of tidy data. Obtained from R for Data Science (Figure 12.1) 3.1 Why tidy data? As discussed in R for Data Science and Tidy Data by Hadley Wickham, tidy data brings 2 main advantages: Consistent data structure means that it will be easier to learn tools that work with it due to the uniformity. The layout of tidy data ensures that the values of each variables from the same observation are always paired. This properties is particularly well suited for vectorized programming languages such as R. 3.2 Common spreadsheets mistakes Common spreadsheets mistakes: Using multiple tables Using multiple tabs Not filling in zeros Using problematic null values Using formatting to convey information Using formatting to make the data sheet look pretty Placing comments or units in cells Entering more than one piece of information in a cell Using problematic field names Using special characters in data Inclusion of metadata in data table Date formatting 3.2.1 Using multiple tables Creating multiple data tables within one spreadsheet confuses the computer as false associations between things are created for the computer, which interpret each row as an observation. Same field name is also probably being used in multiple places, which will make it harder for you to tidy the data into a usable form. In the example above, the computer will see rrow 8 and assume that all columns A-L refer to the same sample. However, this row represents 2 distinct samples (Charcotia obesa from fieldNumber S3 and Abyssorchomene charcoti from fieldNumber S6), as well as some calculated summary statistics (an average (avg) and standard deviation) for species 1 from fieldNumber S3, S4 and S5. library(readxl) library(here) library(tidyverse) # extract row where fieldNumber == S3 read_excel(here(&quot;examples/tidy-data_spreadsheets/tidy-data_multi-tables.xlsx&quot;)) %&gt;% filter(fieldNumber == &quot;S3&quot;) 3.2.2 Using multiple tabs When extra tabs are created, computer will not be able to see the connections in the data that are there (you have to introduce spreadsheet application-specific functions or scripting to ensure this connection). For instance, separate tab is created for each day you take a measurement. This isn’t good practice for two reasons: Inconsistencies are more likely to be introduced into your data Extra step is needed to combine all data from separate tabs into a single data table before analysis. You will have to explicitly tell the computer how to combine tabs - and if the tabs are inconsistently formatted, you might even have to do it manually. 3.2.3 Not filling in zeros There’s a difference between a zero and a blank cell in a spreadsheet to the computer: a zero is an actual data/value. You measured or counted it. A blank cell is a null (missing) value, meaning that it wasn’t measured. The spreadsheets or statistical programs will likely misinterpret blank cells that you intend to be zeros. By not entering the value of your observation, you are telling your computer to represent that data as unknown or missing (null). This can cause problems with subsequent calculations or analyses. For example, the average of a set of numbers which includes a single null value is always null (because the computer can’t guess the value of the missing observations). Because of this, it’s very important to record zeros as zeros and truly missing data as nulls. 3.2.4 Using problematic null values Missing (null) values should be represented consistently throughout the dataset and should be differentiated from the value 0. White et al. presented great examples of different types of commonly used null values, the problems imposed by the types of null values, compatibility with programming languages and their recommendations in Nine simple ways to make it easier to (re)use your data. in Data Sharing in Ecology and Evolution. Table: Null values are indicated as being a null value for specific software if they work consistently and correctly with that software. For example, the null value “NULL” works correctly for certain applications in R, but does not work in others, so it is not presented in the table as R compatible. There are a few reasons why null values get represented differently within a dataset. Confusing null values are automatically recorded from the measuring device. If that’s the case, there’s not much you can do, but it can be addressed in data cleaning before analysis. Different null values are used to convey different reasons why the data is missing. Following tidy data principles, it is better to create a new column like ‘data_missing’ and use that column to capture the different reasons. 3.2.5 Using formatting to convey information Most of the time, formatting such as highlighting could be encoded into a separate column. Example: highlighting cells, rows or columns that encode certain information, leaving blank rows to indicate separations in data. Solution: create a new field to encode the life stage of each occurrence record. 3.2.6 Using formatting to make the data sheet look pretty Example: Cell merging Merging cells could confuse the computer to see associations in data. Merged cells will make the data unreadable by statistical software. read_excel(here(&quot;examples/tidy-data_spreadsheets/tidy-data_merge-cells.xlsx&quot;)) Note that when reading the spreadsheet, the value of merged cells is only interpreted by the function once and assigned to the first record. Hence it is advisable to have the data in each row. read_excel(here(&quot;examples/tidy-data_spreadsheets/tidy-data_merge-cells_correct.xlsx&quot;)) 3.2.7 Placing comments or units in cells Example: A coordinate of a field site was not recorded from GPS device while in the field. The coordinate is later on included the data with a comment to indicate that the coordinate is obtained from a map. read_excel(here(&quot;examples/tidy-data_spreadsheets/tidy-data_comment-cells.xlsx&quot;)) Note that the comment is not visible in the data when read by R. Solution: Excel comments are not readable by most analysis software. To resolve this, create another field in the table to add notes or flag this cells. read_excel(here(&quot;examples/tidy-data_spreadsheets/tidy-data_comment-to-column.xlsx&quot;)) Similarly, do not include units in the same cell that contains the measurement value. Ideally, all measurements taken should be in the same unit. If for some reason they aren’t, create another field to specify the unit. 3.2.8 Entering more than one piece of information in a cell Example: Recording coordinates in a single column. read_excel(here(&quot;examples/tidy-data_spreadsheets/tidy-data_multi-value-per-cell.xlsx&quot;)) Solution: While it is tempting to record a coordinate in a column, it is not a good practice for several reasons: software such as excel might interpret it as a formula and subtract the 2 negative values in coordinate column. inconsistencies such as different delimiters and the order of values maybe introduced while entering the data. the field will have to be split for tools that work with tidy data. Including multiple values in a cell limits the ways in which you can analyze your data. It is better to split the field coordinate into decimalLatitude and decimalLongitude as shown in the figure below. 3.2.9 Using problematic field names Problematic field names include field names which has one or more of the followings: character reason spaces maybe misinterpreted as delimiters numbers some programs don’t like field names that are text strings that start with numbers. Some R packages append an X at the beginning of field name if it starts with number. special characters ($, @, %, #, &amp;, *, (, ), !, /, etc.) often have special meaning in programming languages For instance, importing a spreadsheet with field names that begin with number, contain spaces and special character as shown in figure below with read.xlsx function from xlsx package will lead to a change in field names. library(xlsx) read.xlsx(here(&quot;examples/tidy-data_spreadsheets/tidy-data_problematic-field-names.xlsx&quot;), sheetIndex = 1) We recommend to use underscores (_) as alternative to spaces or consider writing field names in camel case (e.g.: exampleFileName) to improve readability. Abbreviations might make sense temporarily but it maybe hard to recalled after a prolonged period of time. Units can be included in field name to avoid confusion. (e.g. minimumDepthInMeters) Good name Good alternative Avoid max_temp_in_celsius maxTempInCelsius Maximum Temp (°C) decimal_latitude decimalLatitude lat mean_year_growth meanYearGrowth Mean growth/year sex sex M/F 3.2.10 Using special characters in data Using spreadsheet as a word processor when entering data by including line breaks, em-dashes, non-standard characters such as (left- or right-aligned quotation marks) not only makes the data less readable, The data looks fine in the excel file above. However when reading the file into excel, following happens. The carriage of the label becomes \\r\\n_x000D_ when read by R. special_char &lt;- read_excel(here(&quot;examples/tidy-data_spreadsheets/tidy-data_special-char.xlsx&quot;)) special_char When the data frame is being written into a tab separated value file and read back into R, \\r\\n is treated as new line when writing the file. library(tidyverse) file_path &lt;- here(&quot;examples/tidy-data_written/tidy-data_special-char.txt&quot;) write_tsv(special_char, file_path) read_tsv(file_path) 3.2.11 Inclusion of metadata in data table Clear field names of a data table is a good practice, however the amount of information that it can hold is limited and over time, the information will be lost if it is not well documented. The chances that you will remember what the field names means after a prolonged period of time is slim. On the other hand, a good documentation helps the others to understand your data, verify your findings and to review your works. However, the data about your data (metadata) should not be contained in the data file itself because this information is not data. Including it within the same file could disrupt how computer programs interpret your data file. It is better to store this metadata in a separate file in the same directory as your data file, preferably with a name that clearly associates it with your data file. Additionally, file or database level metadata should contains information below: relationship between files that make up the dataset format of the files whether they supercede or are superceded by previous files 3.2.12 Date formatting If you are using spreadsheet program such as microsoft excel to manage date data, it is important to understand how Excel treats this piece of information. Data Carpentry has a chapter that explains this very well. To summarize, dates are stored as number of days (integers) from a default of December 31, 1899 or December 31, 1903 in Excel. It has its pros and cons but because different version of Excel could have different default values and formats, we recommend the following format when storing dates in excel. Split the date into year, month, day or storing dates as a single string in the form of YYYYMMDD. year month day 2021 08 26 startDate endDate 20210801 20210826 "],["data-quality.html", "Chapter 4 Data Quality 4.1 Applications we love 4.2 Geospatial", " Chapter 4 Data Quality There are numerous ways to check your data. Throughout our experience, we gather some tools that we used when we check the quality of biodiversity data that we received. Please note that the tools will change over time. 4.1 Applications we love 4.1.1 Dataset overview Open Refine OpenRefine does not change your original data. All actions are logged in OpenRefine and can be easily reversed. Since the actions are logged and data is not modified, the actions can be easily reproduced. OpenRefine has many more features. Data Carpentry has an excellent lesson on using OpenRefine to clean data for ecologists. 4.1.2 Taxa WoRMS Taxon match We work predominantly with marine data from Southern Ocean. WoRMS species information backbone is integrated into GBIF backbone taxonomy and is the taxonomic backbone used by OBIS. Matching the scientific name to WoRMS before publishing the biodiversity data improves its interoperability. 4.2 Geospatial QGIS QGIS is a free and open source geographic information system with simple graphical user interface. Visualising records Quantarctica "],["useful-online-resources.html", "Chapter 5 Useful online resources 5.1 Courses and webinars 5.2 Best practices 5.3 Others", " Chapter 5 Useful online resources 5.1 Courses and webinars https://datacarpentry.org/lessons/#ecology-workshop https://www.biodiversity.aq/how-to/webinar-series-biodiversity-data-field-research/ https://inbo.github.io/coding-club/sessions/index.html https://ourcodingclub.github.io/course 5.2 Best practices https://data-blog.gbif.org/ https://obis.org/manual/ https://docs.gbif.org/georeferencing-best-practices/1.0/en/ https://docs.gbif.org/sensitive-species-best-practices/master/en/ https://ioos.github.io/bio_data_guide/intro.html 5.3 Others https://www.gbif.org/resource/search?contentType=tool "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
