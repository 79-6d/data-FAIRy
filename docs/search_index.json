[["index.html", "Data FAIRy About How to build this book locally", " Data FAIRy Anton Van de Putte, Yi-Ming Gan, Maxime Sweetlove 2021-06-22 About This document serves to guide researchers who wish to manage the biological data from their scientific expedition Issues or suggestions about the content can be raised through the issue tracker (GitHub account required). How to build this book locally This material has been written using bookdown and R. The dependencies of this project is managed using renv. To build the book locally, clone the repo. If dependencies are not automatically installed by renv when you open data-fairy.Rproj, try the following command. renv::restore() If necessary, update the packages in renv to the latest version by running renv::update() To snapshot the state of project library to the lockfile, run: renv::snapshot() then run the following lines to build the book: library(here) bookdown::render_book(here(&quot;index.Rmd&quot;)) And view it with: browseURL(&quot;docs/index.html&quot;) "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction "],["organization.html", "Chapter 2 Organization 2.1 Project structure 2.2 Organizational tips 2.3 Naming files", " Chapter 2 Organization This section include key points from the following resources: Jenny Bryan’s slides about organization Informative file’s name and location tells you its content, why it exists and its relationship with other things. README’s are great, but if it can be made self-documenting, it does not need to be documented. 2.1 Project structure A good project structure encourages practice that make it easier to come back to old work and make the code easily reproducible. An examples of good project structure adapted from Python’s version from Cookiecutter Data Science is shown below. ├── README.md &lt;- The top-level README. ├── data │ ├── external &lt;- Data from third party sources. │ ├── interim &lt;- Intermediate data that has been transformed. │ ├── processed &lt;- The final, canonical data sets for modeling. │ └── raw &lt;- The original, immutable data dump. │ ├── references &lt;- Data dictionaries, manuals, and all other explanatory materials. │ ├── src &lt;- Source code for use in this project. │ ├── data &lt;- Scripts to download or generate data. │ │ └── make_dataset.R │ │ │ ├── clean &lt;- Scripts to clean data. │ │ └── clean_dataset.R │ │ │ └── visualization &lt;- Scripts to create exploratory and results oriented visualizations │ └── visualize.R └── reports &lt;- Generated QC reports. └── figures &lt;- Generated graphics and figures to be used in reporting. 2.2 Organizational tips A couple of tips from Jenny Bryan’s organization slides which worth to be mentioned: A quarantine directory Revoke write permission to raw data files A prose directory File organization reflects input vs outputs and the information flow 2.2.1 A quarantine directory If your collaborator send you data with space-containing file names, data in spreadsheet etc that do not fits your standard naming system and practice, you can place those files in a quarantine directory. The renamed or exported plain text files can be move to your data directory. Record what you did in a README or comments in your code to remind yourself about the file’s source, if it is from the outside world in a state that is not ready for your programmatic analysis. 2.2.2 Revoke write permission to raw data files So that it will not be accidentally edited by you or someone else. 2.2.3 A prose directory Sometimes you need a folder to keep key emails, internal documentation, explanations, random documents received. Similar to the quarantine directory, the prose directory can be used to park these things without having to keep the same standard for file names and open formats. 2.2.4 File organization should reflect input vs outputs and the information flow TODO add a figure 2.3 Naming files Jenny Bryan also has good tips in naming files summarized below. Good file names are: machine readable human readable orderable Good names 2021-05-20_antarctic-penguins.txt 001-248231_myctophidae-gill.jpg southern-ocean-jellyfish.docx Bad names 1.txt thesis final_Final_FINAL.docx nJ7UyiE*.txt ça va.txt 2.3.1 Machine readable Regular expression and globbing friendly Avoid: special characters spaces punctuation accented characters case sensitivity Use: delimiters File names follow the pattern: &lt;date&gt;_&lt;cruise-number&gt;_&lt;location&gt;.pdf _ underscore delimits units of metadata - hyphen delimits words for readability ~/Desktop/projects/01_cruise-reports ᐅ ls 2018-12-15_PS117_cape-town.pdf 2019-02-09_PS118_punta-arenas.pdf 2019-04-13_PS119_punta-arenas.pdf 2020-06-04_PS122-4_arctic-ocean.pdf 2020-08-12_PS122-5_arctic-ocean.pdf Using globbing/regular expression to narrow file listing: ~/Desktop/projects/01_cruise-reports ᐅ ls *arctic* 2020-06-04_PS122-4_arctic-ocean.pdf 2020-08-12_PS122-5_arctic-ocean.pdf 2.3.2 Human readable Names that tells you about the file content. Utilize the slug concept from semantic URLs. 2019-01-24_gis_vector.pdf 2019-02-12_gis_maps.pdf 2019-03-16_r_ggplot.pdf 2019-04-23_r_dplyr.pdf 2.3.3 Orderable File names that start with numbers. ISO 8601 standard for dates. left pad other numbers with zero(s). Meaningful names start with numbers allow files to be sorted chronologically. Note that date is in ISO 8601 standard format (YYYY-MM-DD). 2020-01-14_team-meeting-minutes.txt 2020-02-21_team-meeting-minutes.txt 2020-02-22_managers-meeting-minutes.txt 2020-03-16_team-meeting-minutes.txt If date is in other format such as (DD-MM-YYYY), sorting the files does not provide chronological order of events. 14-01-2020_team-meeting-minutes.txt 16-03-2020_team-meeting-minutes.txt 21-02-2020_team-meeting-minutes.txt 22-02-2020_managers-meeting-minutes.txt If files are not meaningful when ordered with date, they can be named with numeric characters first. For instance, a folder of images to be added into another document following a certain sequence. 001_myctophidae_diaphus-adenomus.jpg 002_myctophidae_diaphus-agassizii.jpg ... 010_myctophidae_diaphus-danae.jpg 011_myctophidae_diaphus-fragilis.jpg If the file names are not left pad with zeros, the order will not be chronological as depicted in the example below. 10_myctophidae_diaphus-danae.jpg 1_myctophidae_diaphus-adenomus.jpg 11_myctophidae_diaphus-fragilis.jpg 2_myctophidae_diaphus-agassizii.jpg "],["tidy-data.html", "Chapter 3 Tidy data 3.1 Why tidy data? 3.2 Symptoms of messy data 3.3 Common spreadsheets mistakes 3.4 References", " Chapter 3 Tidy data This section of the document compiled information from several references: R for Data Science Tidy Data by Hadley Wickham Tidy data is a standard way of mapping the meaning of a dataset to its structure. In tidy data: Columns = variables Rows = observations Cells = data (values) Figure: Rules of tidy data. Obtained from R for Data Science (Figure 12.1) 3.1 Why tidy data? As discussed in R for Data Science and Tidy Data by Hadley Wickham, tidy data brings 2 main advantages: Consistent data structure = easier to learn tools that work with it due to the uniformity The layout of tidy data ensures that the values of each variables from the same observation are always paired. This properties is particularly is well suited for vectorized programming languages like R. 3.2 Symptoms of messy data Based on http://www.jstatsoft.org/v59/i10/paper Column headers are values, not variable names Multiple variables are stored in one column Variables are stored in both rows and columns Multiple types of observational units are stored in the same table A single observational unit is stored in multiple tables 3.3 Common spreadsheets mistakes This section contains content of Data Organization in Spreadsheets for Ecologists from Data Carpentry Karl W. Broman &amp; Kara H. Woo (2018) Data Organization in Spreadsheets, The American Statistician, 72:1, 2-10, DOI: 10.1080/00031305.2017.1375989 Using multiple tables Using multiple tabs Not filling in zeros Using problematic null values Using formatting to convey information Using formatting to make the data sheet look pretty Placing comments or units in cells Entering more than one piece of information in a cell Using problematic field names Using special characters in data Inclusion of metadata in data table Date formatting 3.3.1 Using multiple tables Creating multiple data tables within one spreadsheet confuses the computer as false associations between things are created for the computer, which interpret each row as an observation. Same field name is also probably being used in multiple places, which will make it harder for you to tidy the data into a usable form. Figure: Example of spreadsheet usign multiple tables In the example above, the computer will see (for example) row 8 and assume that all columns A-L refer to the same sample. However, this row represents 2 distinct samples (species 1 from fieldNumber S3 and species 2 from fieldNumber S6), as well as some calculated summary statistics (an average (avg) and standard deviation) for species 1 from fieldNumber S3, S4 and S5. 3.3.2 Using multiple tabs When extra tabs are created, computer will not be able to see the connections in the data that are there (you have to introduce spreadsheet application-specific functions or scripting to ensure this connection). For instance, separate tab is created for each day you take a measurement. This isn’t good practice for two reasons: Inconsistencies are more likely to be introduced into your data Extra step is needed to combine all data from separate tabs into a single data table before analysis. You will have to explicitly tell the computer how to combine tabs - and if the tabs are inconsistently formatted, you might even have to do it manually. Most of the time adding another column to the original spreadsheet could resolve the problem. In the example mentioned, adding a date column could avoid having multiple tabs of measurements for each day. 3.3.3 Not filling in zeros There’s a difference between a zero and a blank cell in a spreadsheet to the computer: a zero is an actual data. You measured or counted it. A blank cell is a null (missing) value, meaning that it wasn’t measured. The spreadsheets or statistical programs will likely misinterpret blank cells that you intend to be zeros. By not entering the value of your observation, you are telling your computer to represent that data as unknown or missing (null). This can cause problems with subsequent calculations or analyses. For example, the average of a set of numbers which includes a single null value is always null (because the computer can’t guess the value of the missing observations). Because of this, it’s very important to record zeros as zeros and truly missing data as nulls. 3.3.4 Using problematic null values Missing (null) values should be represented consistently throughout the dataset and should be differentiated from the value 0. White et al. presented great examples of different types of commonly used null values, the problems imposed by the types of null values, compatibility with programming languages and their recommendations in Nine simple ways to make it easier to (re)use your data. in Data Sharing in Ecology and Evolution. Table: Null values are indicated as being a null value for specific software if they work consistently and correctly with that software. For example, the null value “NULL” works correctly for certain applications in R, but does not work in others, so it is not presented in the table as R compatible. There are a few reasons why null values get represented differently within a dataset. Confusing null values are automatically recorded from the measuring device. If that’s the case, there’s not much you can do, but it can be addressed in data cleaning before analysis. Different null values are used to convey different reasons why the data is missing. Following tidy data principles, it is better to create a new column like ‘data_missing’ and use that column to capture the different reasons. 3.3.5 Using formatting to convey information Most of the time, formatting such as highlighting could be encoded into a separate column. Example: highlighting cells, rows or columns that encode certain information, leaving blank rows to indicate separations in data. Solution: create a new field to encode the life stage of each occurrence record. 3.3.6 Using formatting to make the data sheet look pretty Example: Cell merging Merging cells could confuse the computer to see associations in data. Merged cells will make the data unreadable by statistical software. library(here) library(readxl) read_excel(here(&quot;examples/tidy-data_merge-cells.xlsx&quot;)) Note that when reading the spreadsheet, the value of merged cells (C1, C2) is only interpreted by the function once and assigned to the first record (id = 1). 3.3.7 Placing comments or units in cells Example: A coordinate of a field site was not recorded from GPS device while in the field. The coordinate is later on included the data with a comment to indicate that the coordinate is obtained from a map. Solution: Excel comments are not readable by most analysis software. To resolve this, create another field in the table to add notes or flag this cells. Similarly, do not include units in the same cell that contains the measurement value. Ideally, all measurements taken should be in the same unit. If for some reason they aren’t, create another field to specify the unit. 3.3.8 Entering more than one piece of information in a cell Example: Recording coordinates in a single column. read_excel(here(&quot;examples/tidy-data_multi-value-per-cell.xlsx&quot;)) Solution: While it is tempting to record a coordinate in a column, it is not a good practice for several reasons: software such as excel might interpret it as a formula and subtract the 2 negative values in coordinate column. inconsistencies such as different delimiters and the order of values maybe introduced while entering the data. the field will have to be split for tools that work with tidy data. Including multiple values in a cell limits the ways in which you can analyze your data. It is better to split the field coordinate into decimalLatitude and decimalLongitude as shown in the figure below. 3.3.9 Using problematic field names Problematic field names include field names which has one or more of the followings: character reason spaces maybe misinterpreted as delimiters numbers some programs don’t like field names that are text strings that start with numbers. Some R packages append an X at the beginning of field name if it starts with number. special characters ($, @, %, #, &amp;, *, (, ), !, /, etc.) often have special meaning in programming languages For instance, importing a spreadsheet with field names that begin with number, contain spaces and special character as shown in figure below with read.xlsx function from xlsx package will lead to a change in field names. library(xlsx) sheet &lt;- read.xlsx(here(&quot;examples/tidy-data_problematic-field-names.xlsx&quot;), sheetIndex = 1) sheet We recommend to use underscores (_) as alternative to spaces or consider writing field names in camel case (e.g.: exampleFileName) to improve readability. Abbreviations might make sense temporarily but it maybe hard to recalled after 5 months. Units can be included in field name to avoid confusion. (e.g. minimumDepthInMeters) Good name Good alternative Avoid max_temp_in_celsius MaxTempInCelsius Maximum Temp (°C) decimal_latitude decimalLatitude latitude mean_year_growth meanYearGrowth Mean growth/year sex sex M/F 3.3.10 Using special characters in data 3.3.11 Inclusion of metadata in data table 3.3.12 Date formatting 3.4 References https://r4ds.had.co.nz/tidy-data.html https://jhudatascience.org/tidyversecourse/intro.html#tidy-data https://github.com/swcarpentry/good-enough-practices-in-scientific-computing/blob/gh-pages/good-enough-practices-for-scientific-computing.pdf "],["data-input.html", "Chapter 4 Data input 4.1 Data templates", " Chapter 4 Data input 4.1 Data templates List of biodiversity data templates to record data team can be found in this GitHub repository. Webinar and slides are available at the portal page 4.1.1 Identifiers 4.1.2 Geographical data 4.1.3 Temporal data 4.1.4 Species data 4.1.5 Measurement data "],["data-quality-control.html", "Chapter 5 Data quality control 5.1 Quality control procedures and tools", " Chapter 5 Data quality control Summary workflow with diagram 5.1 Quality control procedures and tools 5.1.1 Identifiers 5.1.2 Geographical data 5.1.3 Temporal data 5.1.4 Species data 5.1.5 Measurement data "],["publish-your-biodiversity-data.html", "Chapter 6 Publish your biodiversity data 6.1 Darwin Core archive 6.2 Multimedia files", " Chapter 6 Publish your biodiversity data Types of data and publication methods. 6.1 Darwin Core archive Publish Darwin Core archive with IPT 6.1.1 Dataset metadata (EML) 6.1.2 Core and extension files 6.2 Multimedia files Where to publish your multimedia files? 6.2.1 Specimen images For non-human specimen images, Zenodo is a good option to host your images because: a Digital Object Identifier (DOI) will be assigned to your upload to make it citeable. versioning is supported huge data storage space and much more Publishing with Zenodo Upload If you have plenty of images, uploading one folder containing all of your images for one specific dataset (corresponds to your dataset that will be published via IPT) is the easiest approach. Please ensure that the file names for your images corresponds to your data record in IPT so that the information is properly linked. Communities If your upload is an Antarctic biodiversity dataset, please associate the following communities to your upload: SCAR Scientific Committee on Antarctic Research SCAR Antarctic Biodiversity Portal It helps us to trace what are the datasets uploaded and make your dataset more findable. Basic information Title Please use the same title as your publication in IPT with a suffix -multimedia, e.g.: My title from IPT-multimedia License We recommend Creative Commons Attribution 4.0 International license for the specimen images, where users are free to share and adapt your multimedia but they must give appropriate credit to you, provide a link to the license and indicate if changes were made. "],["useful-online-resources.html", "Chapter 7 Useful online resources 7.1 Courses and webinars 7.2 Best practices 7.3 Others", " Chapter 7 Useful online resources 7.1 Courses and webinars https://datacarpentry.org/lessons/#ecology-workshop https://www.biodiversity.aq/how-to/webinar-series-biodiversity-data-field-research/ https://inbo.github.io/coding-club/sessions/index.html https://ourcodingclub.github.io/course 7.2 Best practices https://data-blog.gbif.org/ https://obis.org/manual/ https://docs.gbif.org/georeferencing-best-practices/1.0/en/ https://docs.gbif.org/sensitive-species-best-practices/master/en/ https://ioos.github.io/bio_data_guide/intro.html 7.3 Others https://www.gbif.org/resource/search?contentType=tool "]]
